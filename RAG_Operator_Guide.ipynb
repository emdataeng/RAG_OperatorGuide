{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4f4152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RAG MVP for Operator Training (text + images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b9924",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1319fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "from IPython.display import display\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "import hashlib\n",
    "import json, re\n",
    "from math import inf\n",
    "from jinja2 import Environment, FileSystemLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a37a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally, uncomment and run these installs:\n",
    "# !pip install --upgrade pip\n",
    "# !pip install pymupdf pdfminer.six pillow numpy pandas tqdm faiss-cpu\n",
    "# !pip install open_clip_torch torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# Optional (GPU):\n",
    "# !pip install open_clip_torch torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# Optional re-ranking or OCR extras:\n",
    "# !pip install rapidfuzz opencv-python\n",
    "# If you plan to call OpenAI for step generation:\n",
    "# !pip install openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0538a",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4c025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(pdf_dir='data/pdfs', out_dir='artifacts', images_dir_name='images', catalog_json='catalog.jsonl', openclip_model='ViT-B-32', openclip_pretrained='laion2b_s34b_b79k', device='cpu', dims=512, top_k=10, image_boost=1.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    pdf_dir: str = \"data/pdfs\"              # Put your manuals here\n",
    "    out_dir: str = \"artifacts\"              # Extracted assets & indices\n",
    "    images_dir_name: str = \"images\"         # Subfolder for extracted images\n",
    "    catalog_json: str = \"catalog.jsonl\"     # Captured page items (texts/images)\n",
    "\n",
    "    # OpenCLIP model settings\n",
    "    openclip_model: str = \"ViT-B-32\"        # e.g., 'ViT-B-32', 'ViT-bigG-14'\n",
    "    openclip_pretrained: str = \"laion2b_s34b_b79k\"\n",
    "    device: str = \"cpu\"                     # 'cuda' if available\n",
    "\n",
    "    # Indexing / retrieval\n",
    "    dims: int = 512                         # OpenCLIP text/image output dims (depends on model)\n",
    "    top_k: int = 10                         # Top-k results to return\n",
    "    image_boost: float = 1.0                # Boost factor for image scores\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Prepare paths\n",
    "Path(cfg.pdf_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(cfg.out_dir, cfg.images_dir_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f848a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Settings ---\n",
    "#PDF_PATH = \"data/ManualOp-Modo Manual SIF400_merged_SIF402.pdf\"          # Place PDF here\n",
    "MACHINE_NAME = \"SIF402\"               # Set the machine name\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in env\")\n",
    "\n",
    "PATH_SYSTEM_PROMPT = \"config/system_prompt.txt\"  # Optional system prompt path\n",
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/tmp/hf_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/tmp/hf_cache\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc71b8",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78aedaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def norm_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def safe_fname(s: str) -> str:\n",
    "    s = norm_space(s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", s)\n",
    "    return s[:200]\n",
    "\n",
    "def heading_score(fontsize: float) -> float:\n",
    "    # Heuristic: larger font size -> higher heading score\n",
    "    return fontsize\n",
    "\n",
    "def build_heading_hierarchy(previous_headings: List[Dict[str, Any]], current: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    # Simplistic heuristic based on font sizes: if current fontsize is smaller, it could be a subheading\n",
    "    # Otherwise, it might reset the hierarchy\n",
    "    if not previous_headings:\n",
    "        return [current]\n",
    "    last = previous_headings[-1]\n",
    "    if current[\"fontsize\"] < last[\"fontsize\"]:\n",
    "        return previous_headings + [current]  # deeper level\n",
    "    else:\n",
    "        return [current]  # reset hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16f1ce",
   "metadata": {},
   "source": [
    "## 3. PDF Parsing (text blocks, font sizes, images, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc64449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_filter(bbox, page_height, ignore_top_pct=0.05, ignore_bottom_pct=0.05, min_size=40, image=None):\n",
    "    \"\"\"\n",
    "    Filters out unwanted regions/images:\n",
    "      - skip images too close to top/bottom (logos, footers)\n",
    "      - skip small icons/logos (width or height < min_size)\n",
    "      - skip completely black images\n",
    "    Returns True if the image should be kept.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    width, height = x1 - x0, y1 - y0\n",
    "\n",
    "    # 1️ Skip top/bottom zones\n",
    "    if y1 < ignore_top_pct * page_height or y0 > (1 - ignore_bottom_pct) * page_height:\n",
    "        return False\n",
    "\n",
    "    # 2️ Skip small icons\n",
    "    if width < min_size or height < min_size:\n",
    "        return False\n",
    "\n",
    "    # 3️ Skip fully black images\n",
    "    if image is not None:\n",
    "        arr = np.array(image.convert(\"L\"))  # grayscale\n",
    "        if np.mean(arr) < 5:  # near-black threshold\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751ead3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def extract_pdf(pdf_path: str, out_dir: str, images_dir_name: str,\n",
    "                MACHINE_NAME: str,\n",
    "                ignore_top_pct=0.05, ignore_bottom_pct=0.05, min_size=40) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extracts text blocks and images from a PDF.\n",
    "    Includes MACHINE_NAME in all image metadata and filenames.\n",
    "    Filters images by region_filter() to skip header/footer/small/black ones.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    items = []\n",
    "    img_counter_global = 0\n",
    "\n",
    "    for page_index in range(len(doc)):\n",
    "        page = doc[page_index]\n",
    "        page_height = page.rect.height\n",
    "        page_dict = page.get_text(\"dict\")  # contains blocks, lines, spans with fonts/sizes\n",
    "        blocks = page_dict.get(\"blocks\", [])\n",
    "\n",
    "        # Collect text spans with font sizes and bbox\n",
    "        text_items = []\n",
    "        for b in blocks:\n",
    "            if b[\"type\"] == 0:  # text block\n",
    "                bbox = b[\"bbox\"]\n",
    "                lines = b.get(\"lines\", [])\n",
    "                full_text = \"\"\n",
    "                max_font = 0.0\n",
    "                for ln in lines:\n",
    "                    for sp in ln.get(\"spans\", []):\n",
    "                        full_text += sp[\"text\"]\n",
    "                        if sp.get(\"size\", 0) > max_font:\n",
    "                            max_font = sp[\"size\"]\n",
    "                text_items.append({\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": norm_space(full_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")),\n",
    "                    \"fontsize\": max_font,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"page\": page_index\n",
    "                })\n",
    "\n",
    "            elif b[\"type\"] == 1:  # image block\n",
    "                bbox = b[\"bbox\"]\n",
    "                # extract image by id from page.get_images\n",
    "                # Multiple images may exist; we will grab the one whose bbox matches approximately\n",
    "                # Instead, use doc.extract_image for images listing; safer: use get_pixmap on bbox.\n",
    "                pix = page.get_pixmap(clip=fitz.Rect(*bbox), dpi=150)\n",
    "                img_bytes = pix.tobytes(\"png\")\n",
    "                img = Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "                #apply region filter:\n",
    "                if not region_filter(bbox, page_height,\n",
    "                                     ignore_top_pct=ignore_top_pct,\n",
    "                                     ignore_bottom_pct=ignore_bottom_pct,\n",
    "                                     min_size=min_size,\n",
    "                                     image=img):\n",
    "                    continue  # skip unwanted images\n",
    "\n",
    "                # Save image with machine name prefix\n",
    "                img_counter_global += 1\n",
    "                img_name = f\"{MACHINE_NAME}_p{page_index+1:03d}_img{img_counter_global:04d}.png\"\n",
    "                img_path = os.path.join(out_dir, images_dir_name, img_name)\n",
    "                img.save(img_path)\n",
    "\n",
    "                text_items.append({\n",
    "                    \"type\": \"image\",\n",
    "                    \"image_path\": img_path,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"page\": page_index,\n",
    "                })\n",
    "\n",
    "        # Sort items by vertical position (y0)\n",
    "        text_items.sort(key=lambda x: x[\"bbox\"][1] if \"bbox\" in x else 0.0)\n",
    "\n",
    "        # Build heading contexts and attach nearest preceding headings to images\n",
    "        heading_stack: List[Dict[str, Any]] = []\n",
    "        image_counters_by_heading: Dict[str, int] = {}\n",
    "\n",
    "        for it in text_items:\n",
    "            if it[\"type\"] == \"text\":\n",
    "                # Heuristic: treat as heading if font size is among the largest on page or if it matches numbered pattern\n",
    "                is_numbered = bool(re.match(r\"^\\\\d+(\\\\.\\\\d+)*\\\\s+.+\", it[\"text\"]))\n",
    "                if is_numbered or it[\"fontsize\"] >= (max([t[\"fontsize\"] for t in text_items if t[\"type\"]==\"text\"]+[0])*0.9):\n",
    "                    heading_stack = build_heading_hierarchy(heading_stack, it)\n",
    "                    #Keep only meaningful text\n",
    "                    heading_stack[-1][\"text\"] = it[\"text\"]\n",
    "\n",
    "            elif it[\"type\"] == \"image\":\n",
    "                # Build heading chain text\n",
    "                if heading_stack:\n",
    "                    chain_texts = [h[\"text\"] for h in heading_stack if h.get(\"text\")]\n",
    "                    chain_compact = \" > \".join(chain_texts)\n",
    "                    top_heading = heading_stack[-1][\"text\"]\n",
    "                else:\n",
    "                    chain_texts = []\n",
    "                    chain_compact = \"\"\n",
    "                    top_heading = \"Unlabeled\"\n",
    "\n",
    "                # Create metadata key for counting images under the top heading\n",
    "                top_key = safe_fname(top_heading) if top_heading else \"Unlabeled\"\n",
    "                image_counters_by_heading.setdefault(top_key, 0)\n",
    "                image_counters_by_heading[top_key] += 1\n",
    "                img_idx = image_counters_by_heading[top_key]\n",
    "\n",
    "                # Build user-style metadata name\n",
    "                # Prefer the deepest numbered heading if available\n",
    "                numbered = [t for t in chain_texts if re.match(r\"^\\\\d+(\\\\.\\\\d+)*\\\\s+.+\", t)]\n",
    "                final_heading = numbered[-1] if numbered else top_heading or \"Unlabeled\"\n",
    "\n",
    "                image_metadata_name = f\"{MACHINE_NAME}_{final_heading}_image_{img_idx}\"\n",
    "\n",
    "                items.append({\n",
    "                    \"modality\": \"image\",\n",
    "                    \"page\": it[\"page\"],\n",
    "                    \"bbox\": it[\"bbox\"],\n",
    "                    \"image_path\": it[\"image_path\"],\n",
    "                    \"heading_chain\": chain_texts,\n",
    "                    \"heading_path\": chain_compact,\n",
    "                    \"image_metadata_name\": image_metadata_name,\n",
    "                    \"source_pdf\": pdf_path,\n",
    "                    \"source_machine\": MACHINE_NAME\n",
    "                })\n",
    "\n",
    "        # Record text blocks for text retrieval\n",
    "        for t in text_items:\n",
    "            if t[\"type\"] == \"text\" and t.get(\"text\"):\n",
    "                items.append({\n",
    "                    \"modality\": \"text\",\n",
    "                    \"page\": t[\"page\"],\n",
    "                    \"bbox\": t[\"bbox\"],\n",
    "                    \"text\": t[\"text\"],\n",
    "                    \"fontsize\": t[\"fontsize\"],\n",
    "                    \"source_pdf\": pdf_path,\n",
    "                    \"source_machine\": MACHINE_NAME\n",
    "                })\n",
    "\n",
    "    doc.close()\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd3adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(pdf_dir: str, out_dir: str, images_dir_name: str, catalog_json: str,\n",
    "                    MACHINE_NAME: str, ignore_bottom_pct: float, ignore_top_pct: float):\n",
    "    all_items = []\n",
    "    pdfs = [str(p) for p in Path(pdf_dir).glob(\"**/*.pdf\")]\n",
    "    for p in tqdm(pdfs, desc=f\"Extracting PDFs for {MACHINE_NAME}\"):\n",
    "        items = extract_pdf(p, out_dir, images_dir_name, MACHINE_NAME, ignore_bottom_pct, ignore_top_pct)\n",
    "        all_items.extend(items)\n",
    "    # Write JSONL catalog\n",
    "    out_path = os.path.join(out_dir, catalog_json)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for it in all_items:\n",
    "            line = json.dumps(it, ensure_ascii=False)\n",
    "            line = line.replace(\"\\n\", \"\\\\n\")  # escape accidental newlines inside JSON\n",
    "            f.write(line + \"\\n\")\n",
    "    print(f\"Wrote {len(all_items)} items for {MACHINE_NAME} to {out_path}\")\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b532c7",
   "metadata": {},
   "source": [
    "## 4. OpenCLIP: Model & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbabf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache directory: /tmp/hf_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Confirm effective cache directory\n",
    "print(\"Hugging Face cache directory:\", os.environ[\"HF_HOME\"])\n",
    "\n",
    "# Lazy import to avoid failures if packages aren't installed yet\n",
    "def load_openclip(model_name: str, pretrained: str, device: str=\"cpu\"):\n",
    "    import torch\n",
    "    import open_clip\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "                model_name, pretrained=pretrained, device=device,\n",
    "                cache_dir=os.environ[\"HF_HOME\"]   # ensure it uses the safe cache\n",
    "    )\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    return model, preprocess, tokenizer\n",
    "\n",
    "def embed_texts_openclip(texts: List[str], model, tokenizer, device=\"cpu\", batch_size=32) -> np.ndarray:\n",
    "    import torch\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            tok = tokenizer(batch)\n",
    "            tok = {k: v.to(device) for k,v in tok.items()} if isinstance(tok, dict) else tok.to(device)\n",
    "            feats = model.encode_text(tok)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            embs.append(feats.cpu().numpy())\n",
    "    return np.vstack(embs) if embs else np.zeros((0,))\n",
    "\n",
    "def embed_images_openclip(img_paths: List[str], model, preprocess, device=\"cpu\", batch_size=16) -> np.ndarray:\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(img_paths), batch_size):\n",
    "            batch = img_paths[i:i+batch_size]\n",
    "            imgs = []\n",
    "            for p in batch:\n",
    "                im = Image.open(p).convert(\"RGB\")\n",
    "                imgs.append(preprocess(im))\n",
    "            imgs = torch.stack(imgs).to(device)\n",
    "            feats = model.encode_image(imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            embs.append(feats.cpu().numpy())\n",
    "    return np.vstack(embs) if embs else np.zeros((0,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3e891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############Captions#############\n",
    "# Build a textual surrogate (caption) since images in pdfs don't come with captions\n",
    "import pandas as pd\n",
    "def build_caption_for_images(df_img: pd.DataFrame) -> List[str]:\n",
    "    # Build a textual surrogate (caption)\n",
    "    captions = []\n",
    "    for rec in df_img.to_dict(orient=\"records\"):\n",
    "        # Combine semantic hints into a short phrase\n",
    "        meta_parts = [\n",
    "            rec.get(\"image_metadata_name\") or \"\",\n",
    "            rec.get(\"heading_path\") or \"\",\n",
    "        ]\n",
    "        caption = \" | \".join(p for p in meta_parts if p)\n",
    "        captions.append(caption if caption.strip() else \"image from manual\")\n",
    "    return captions\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d40be",
   "metadata": {},
   "source": [
    "## 5. Build a Multimodal FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf1ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def json_parser(catalog_path: str) -> pd.DataFrame:\n",
    "    # Load catalog\n",
    "    rows = []\n",
    "    with open(catalog_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            #Try safe parsing — skip malformed lines\n",
    "            try:\n",
    "                # Some lines may accidentally contain multiple JSON objects -> split them\n",
    "                parts = re.findall(r'\\{.*?\\}(?=$|\\s*\\{)', line)\n",
    "                if len(parts) > 1:\n",
    "                    for p in parts:\n",
    "                        rows.append(json.loads(p))\n",
    "                else:\n",
    "                    rows.append(json.loads(line))\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skipping bad JSON line {i}: {str(e)[:80]}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Loaded {len(rows)} valid JSON objects from {catalog_path}\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(df.head(2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbad95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "def build_indices(catalog_path: str, cfg: Config):\n",
    "    # Load catalog    \n",
    "    df = json_parser(catalog_path)\n",
    "\n",
    "    # Separate modalities\n",
    "    df_text = df[df[\"modality\"]==\"text\"].copy()\n",
    "    df_img = df[df[\"modality\"]==\"image\"].copy()\n",
    "\n",
    "    # Load OpenCLIP\n",
    "    model, preprocess, tokenizer = load_openclip(cfg.openclip_model, cfg.openclip_pretrained, cfg.device)\n",
    "\n",
    "    # Prepare contents for embeddings\n",
    "    text_payloads = df_text[\"text\"].tolist()\n",
    "    img_paths = df_img[\"image_path\"].tolist()\n",
    "\n",
    "    print(f\"Embedding {len(text_payloads)} text blocks and {len(img_paths)} images...\")\n",
    "    text_embs = embed_texts_openclip(text_payloads, model, tokenizer, cfg.device)\n",
    "    img_embs = embed_images_openclip(img_paths, model, preprocess, cfg.device)\n",
    "\n",
    "\n",
    "###########New###########################\n",
    "    # Create text embeddings for captions using the same text encoder\n",
    "    captions = build_caption_for_images(df_img)\n",
    "    text_capt_embs = embed_texts_openclip(captions, model, tokenizer, cfg.device)\n",
    "\n",
    "    # Normalize both before fusing\n",
    "    img_embs = img_embs / np.linalg.norm(img_embs, axis=1, keepdims=True)\n",
    "    text_capt_embs = text_capt_embs / np.linalg.norm(text_capt_embs, axis=1, keepdims=True)\n",
    "\n",
    "    # Weighted fusion (adjust alpha to taste)\n",
    "    alpha = 0.5  # try 0.3–0.7 range\n",
    "    emb_fused = (1 - alpha) * img_embs + alpha * text_capt_embs\n",
    "    emb_fused = emb_fused / np.linalg.norm(emb_fused, axis=1, keepdims=True)\n",
    "\n",
    "    # 5. Use emb_fused for indexing\n",
    "    #index_image.add(emb_fused.astype(\"float32\"))\n",
    "\n",
    "    print(\"Image-only mean sim:\", np.mean(img_embs @ text_embs.T))\n",
    "    print(\"Fused mean sim:\", np.mean(emb_fused @ text_embs.T))\n",
    "\n",
    "    # Replace img_embs with fused embeddings\n",
    "    img_embs = emb_fused\n",
    "    ########################################\n",
    "\n",
    "\n",
    "\n",
    "    # Build FAISS indices (cosine similarity via inner product on normalized vectors)\n",
    "    dim_text = text_embs.shape[1] if len(text_embs.shape)==2 else cfg.dims\n",
    "    dim_img  = img_embs.shape[1] if len(img_embs.shape)==2 else cfg.dims\n",
    "\n",
    "    index_text = faiss.IndexFlatIP(dim_text)\n",
    "    index_img  = faiss.IndexFlatIP(dim_img)\n",
    "\n",
    "    if len(text_embs):\n",
    "        index_text.add(text_embs.astype(\"float32\"))\n",
    "    if len(img_embs):\n",
    "        index_img.add(img_embs.astype(\"float32\"))\n",
    "\n",
    "    # Persist\n",
    "    faiss.write_index(index_text, os.path.join(cfg.out_dir, \"faiss_text.index\"))\n",
    "    faiss.write_index(index_img,  os.path.join(cfg.out_dir, \"faiss_image.index\"))\n",
    "    df_text.to_json(os.path.join(cfg.out_dir, \"df_text.json\"), orient=\"records\", lines=True)\n",
    "    df_img.to_json(os.path.join(cfg.out_dir, \"df_img.json\"), orient=\"records\", lines=True)\n",
    "\n",
    "    print(\"Indices built and saved.\")\n",
    "    return {\n",
    "        \"index_text_path\": os.path.join(cfg.out_dir, \"faiss_text.index\"),\n",
    "        \"index_image_path\": os.path.join(cfg.out_dir, \"faiss_image.index\"),\n",
    "        \"df_text_path\": os.path.join(cfg.out_dir, \"df_text.json\"),\n",
    "        \"df_image_path\": os.path.join(cfg.out_dir, \"df_img.json\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b266d",
   "metadata": {},
   "source": [
    "## 6. Retrieval & Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6061c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "################New\n",
    "# --- Normalize scores per modality\n",
    "def normalize_scores(recs):\n",
    "    if not recs: return recs\n",
    "    vals = np.array([r[\"score\"] for r in recs], dtype=float)\n",
    "    mu, sigma = vals.mean(), vals.std() + 1e-6\n",
    "    for r in recs:\n",
    "        r[\"score_norm\"] = (r[\"score\"] - mu) / sigma\n",
    "    return recs\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "220f3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indices(cfg: Config):\n",
    "    import pandas as pd\n",
    "    index_text = faiss.read_index(os.path.join(cfg.out_dir, \"faiss_text.index\"))\n",
    "    index_image = faiss.read_index(os.path.join(cfg.out_dir, \"faiss_image.index\"))\n",
    "    df_text = pd.read_json(os.path.join(cfg.out_dir, \"df_text.json\"), orient=\"records\", lines=True)\n",
    "    df_img  = pd.read_json(os.path.join(cfg.out_dir, \"df_img.json\"),  orient=\"records\", lines=True)\n",
    "    model, preprocess, tokenizer = load_openclip(cfg.openclip_model, cfg.openclip_pretrained, cfg.device)\n",
    "    return index_text, index_image, df_text, df_img, model, preprocess, tokenizer\n",
    "\n",
    "def search(query: str, cfg: Config, top_k: int=None) -> Dict[str, Any]:\n",
    "    import torch\n",
    "    top_k = top_k or cfg.top_k\n",
    "    index_text, index_image, df_text, df_img, model, preprocess, tokenizer = load_indices(cfg)\n",
    "\n",
    "    # Embed query\n",
    "    q_emb = embed_texts_openclip([query], model, tokenizer, cfg.device)[0].astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "    # Search text\n",
    "    D_t, I_t = index_text.search(q_emb, top_k)\n",
    "    # Search images\n",
    "    D_i, I_i = index_image.search(q_emb, top_k)\n",
    "\n",
    "    # Combine with simple late fusion (scores already cosine similarities)\n",
    "    #results = []\n",
    "    text_hits = []\n",
    "    img_hits = []\n",
    "    \n",
    "    for score, idx in zip(D_t[0].tolist(), I_t[0].tolist()):\n",
    "        if idx == -1: continue\n",
    "        rec = df_text.iloc[idx].to_dict()\n",
    "        rec.update({\"score\": float(score), \"modality\": \"text\"})\n",
    "        ##results.append(rec)\n",
    "        text_hits.append(rec)\n",
    "\n",
    "    for score, idx in zip(D_i[0].tolist(), I_i[0].tolist()):\n",
    "        if idx == -1: continue\n",
    "        rec = df_img.iloc[idx].to_dict()\n",
    "        rec.update({\"score\": float(score * cfg.image_boost), \"modality\": \"image\"})\n",
    "        #results.append(rec)\n",
    "        img_hits.append(rec)\n",
    "\n",
    "    # Sort by score desc and take top_k overall\n",
    "    #results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    #results = results[:top_k]\n",
    "    \n",
    "    # --- Normalize scores per modality\n",
    "    text_hits = normalize_scores(text_hits)\n",
    "    img_hits = normalize_scores(img_hits)\n",
    "\n",
    "    # --- Merge and sort by normalized score ---\n",
    "    merged = text_hits + img_hits\n",
    "    merged.sort(key=lambda x: x[\"score_norm\"], reverse=True)\n",
    "    results = merged[:top_k]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d20a2",
   "metadata": {},
   "source": [
    "## 7. JSON Output Composer (Steps + Relevant Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15daec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_steps_from_text(text: str) -> List[str]:\n",
    "    # Simple heuristic: split on numbered or bulleted lines; clean\n",
    "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "    steps = []\n",
    "    for ln in lines:\n",
    "        if re.match(r\"^(?:\\d+\\.|[-*•])\\s+\", ln):\n",
    "            steps.append(re.sub(r\"^(?:\\d+\\.|[-*•])\\s+\", \"\", ln).strip())\n",
    "    # Fallback: if no explicit bullets, chunk sentences\n",
    "    if not steps:\n",
    "        sents = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        steps = [s.strip() for s in sents if len(s.strip()) > 0][:6]\n",
    "    return steps[:8]\n",
    "\n",
    "def compose_json_plan(query: str, search_out: Dict[str, Any], max_images: int = 4) -> Dict[str, Any]:\n",
    "    # Gather top text to form step suggestions, and top images to include\n",
    "    texts = [r[\"text\"] for r in search_out[\"results\"] if r[\"modality\"]==\"text\" and isinstance(r.get(\"text\"), str)]\n",
    "    big_context = \"\\n\".join(texts[:5]) if texts else \"\"\n",
    "    steps = extract_steps_from_text(big_context) if big_context else []\n",
    "\n",
    "    img_hits = [r for r in search_out[\"results\"] if r[\"modality\"]==\"image\"]\n",
    "    img_hits = img_hits[:max_images]\n",
    "\n",
    "    images_payload = []\n",
    "    for r in img_hits:\n",
    "        images_payload.append({\n",
    "            \"path\": r.get(\"image_path\"),\n",
    "            \"page\": int(r.get(\"page\", -1)),\n",
    "            \"bbox\": r.get(\"bbox\"),\n",
    "            \"heading_path\": r.get(\"heading_path\"),\n",
    "            \"image_metadata_name\": r.get(\"image_metadata_name\"),\n",
    "            \"source_pdf\": r.get(\"source_pdf\")\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"steps\": steps,\n",
    "        \"images\": images_payload,\n",
    "        \"context_used_charlen\": len(big_context)\n",
    "    }\n",
    "\n",
    "# Example usage (after building the index):\n",
    "# out = search(\"Emergency stop procedure\", cfg, top_k=10)\n",
    "# plan = compose_json_plan(\"Emergency stop procedure\", out, max_images=4)\n",
    "# plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9d418",
   "metadata": {},
   "source": [
    "## 8. End-to-End Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9586eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run:\n",
      "1) Put PDFs in: data/pdfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PDFs for SIF02: 100% 1/1 [00:06<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 591 items for SIF02 to artifacts/catalog.jsonl\n",
      "Loaded 591 valid JSON objects from artifacts/catalog.jsonl\n",
      "  modality  page                                               bbox  \\\n",
      "0    image     0  [38.375003814697266, 164.77999877929688, 80.85...   \n",
      "1     text     0  [352.1499938964844, 210.88999938964844, 466.23...   \n",
      "\n",
      "                                image_path heading_chain heading_path  \\\n",
      "0  artifacts/images/SIF02_p001_img0001.png            []                \n",
      "1                                      NaN           NaN          NaN   \n",
      "\n",
      "       image_metadata_name                                         source_pdf  \\\n",
      "0  SIF02_Unlabeled_image_1  data/pdfs/ManualOp-Modo Manual SIF400_merged_S...   \n",
      "1                      NaN  data/pdfs/ManualOp-Modo Manual SIF400_merged_S...   \n",
      "\n",
      "  source_machine         text  fontsize  \n",
      "0          SIF02          NaN       NaN  \n",
      "1          SIF02  Manual mode      18.0  \n",
      "Embedding 514 text blocks and 77 images...\n",
      "Image-only mean sim: 0.18640211\n",
      "Fused mean sim: 0.47254828\n",
      "Indices built and saved.\n",
      "2) {'index_text_path': 'artifacts/faiss_text.index', 'index_image_path': 'artifacts/faiss_image.index', 'df_text_path': 'artifacts/df_text.json', 'df_image_path': 'artifacts/df_img.json'} # may take time\n",
      "3) {'query': \"I see in the panel in SIF402 a message: 'Please refill red pellets', what should I do?\", 'steps': ['Pellets', '“Pellet presence\": there are pellets present.', '“Min level of pellet\": no pellets present.'], 'images': [{'path': 'artifacts/images/SIF02_p038_img0069.png', 'page': 37, 'bbox': [122.1500015259, 434.8299865723, 473.6099853516, 662.6799926758], 'heading_path': 'pellets required for production.', 'image_metadata_name': 'SIF02_pellets required for production._image_1', 'source_pdf': 'data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf'}, {'path': 'artifacts/images/SIF02_p026_img0038.png', 'page': 25, 'bbox': [132.6000061035, 587.7600097656, 507.240020752, 630.3599853516], 'heading_path': '1.8 Planned stops', 'image_metadata_name': 'SIF02_1.8 Planned stops_image_2', 'source_pdf': 'data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf'}, {'path': 'artifacts/images/SIF02_p032_img0049.png', 'page': 31, 'bbox': [86.5500030518, 333.1399536133, 509.549987793, 588.3199462891], 'heading_path': 'Checking the availability of raw materials', 'image_metadata_name': 'SIF02_Checking the availability of raw materials_image_1', 'source_pdf': 'data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf'}, {'path': 'artifacts/images/SIF02_p037_img0063.png', 'page': 36, 'bbox': [98.8249969482, 485.3199462891, 496.325012207, 567.0699462891], 'heading_path': 'because there are not enough pellets of the colour indicated. To continue, fill the corresponding hopper with pellets. To finish, press the RESET button on the main screen of the HMI panel.', 'image_metadata_name': 'SIF02_because there are not enough pellets of the colour indicated. To continue, fill the corresponding hopper with pellets. To finish, press the RESET button on the main screen of the HMI panel._image_1', 'source_pdf': 'data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf'}], 'context_used_charlen': 190}\n"
     ]
    }
   ],
   "source": [
    "def build_all(cfg: Config):\n",
    "    catalog_path = os.path.join(cfg.out_dir, cfg.catalog_json)\n",
    "    _ = run_extraction(cfg.pdf_dir, cfg.out_dir, cfg.images_dir_name, cfg.catalog_json,\n",
    "                       MACHINE_NAME = \"SIF02\", ignore_bottom_pct=0.1, ignore_top_pct=0.1)\n",
    "    idx_paths = build_indices(catalog_path, cfg)\n",
    "    return idx_paths\n",
    "\n",
    "def retrieve_plan(query: str, cfg: Config, top_k: int=10, max_images: int=4):\n",
    "    search_out = search(query, cfg, top_k=top_k)\n",
    "    plan = compose_json_plan(query, search_out, max_images=max_images)\n",
    "    return plan\n",
    "\n",
    "print(\"To run:\")\n",
    "print(\"1) Put PDFs in:\", cfg.pdf_dir)\n",
    "print(f\"2) {build_all(cfg)} # may take time\")\n",
    "#print(f\"3) {retrieve_plan('Emergency stop procedure for station X', cfg)}\")\n",
    "query =f\"I see in the panel in {MACHINE_NAME} a message: 'Please refill red pellets', what should I do?\"\n",
    "print(f\"3) {retrieve_plan(query, cfg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ee6ca",
   "metadata": {},
   "source": [
    "## 9. Build a context that merges text + image info\n",
    "\n",
    "It creates a single human-readable context string summarizing everything retrieved for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55b555f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_from_plan_(plan: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build a readable text context for the LLM that includes:\n",
    "    - Textual steps or context from retrieved text\n",
    "    - References to relevant images with their heading paths and filenames\n",
    "    \"\"\"\n",
    "    ctx = []\n",
    "    ctx.append(f\"Query: {plan['query']}\\n\")\n",
    "\n",
    "    # Add textual steps or content\n",
    "    if plan.get(\"steps\"):\n",
    "        ctx.append(\"Relevant text snippets / steps found:\\n\")\n",
    "        for i, step in enumerate(plan[\"steps\"], start=1):\n",
    "            ctx.append(f\"  {i}. {step}\")\n",
    "\n",
    "    # Add image metadata\n",
    "    if plan.get(\"images\"):\n",
    "        ctx.append(\"\\nRelevant images:\\n\")\n",
    "        for img in plan[\"images\"]:\n",
    "            heading = img.get(\"heading_path\", \"Unlabeled\")\n",
    "            name = img.get(\"image_metadata_name\", os.path.basename(img.get(\"path\", \"\")))\n",
    "            path = img.get(\"path\", \"\")\n",
    "            ctx.append(\n",
    "                f\"  - {heading} \"\n",
    "                f\"(metadata name: {img.get('image_metadata_name')}) \"\n",
    "                f\"-> file: {os.path.basename(path)}\"\n",
    "            )\n",
    "\n",
    "    return \"\\n\".join(ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "400932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_context_from_plan(plan: dict, base_path: str = \".\") -> str:\n",
    "    \"\"\"\n",
    "    Build a readable text context for the LLM that includes:\n",
    "    - Textual context / steps retrieved\n",
    "    - References to relevant images with true metadata names and relative paths\n",
    "    \n",
    "    Args:\n",
    "        plan: dict returned by retrieve_plan()\n",
    "        base_path: root folder (e.g. project Basepath), used to resolve image paths\n",
    "    \n",
    "    Returns:\n",
    "        str: context string for LLM prompt\n",
    "    \"\"\"\n",
    "    ctx = []\n",
    "    ctx.append(f\"Query: {plan.get('query', '')}\\n\")\n",
    "\n",
    "    # ---- TEXT CONTEXT ----\n",
    "    if plan.get(\"steps\"):\n",
    "        ctx.append(\"Relevant textual context or steps found:\\n\")\n",
    "        for i, step in enumerate(plan[\"steps\"], start=1):\n",
    "            ctx.append(f\"  Step {i}: {step}\")\n",
    "    else:\n",
    "        ctx.append(\"No textual steps found in retrieval.\\n\")\n",
    "\n",
    "    # ---- IMAGE CONTEXT ----\n",
    "    images = plan.get(\"images\", [])\n",
    "    if images:\n",
    "        ctx.append(\"\\nRelevant images found in manuals:\\n\")\n",
    "        for i, img in enumerate(images, start=1):\n",
    "            heading = img.get(\"heading_path\", \"Unlabeled section\")\n",
    "            meta_name = img.get(\"image_metadata_name\", \"Unknown_image\")\n",
    "            rel_path = img.get(\"path\", \"\")\n",
    "            abs_path = os.path.abspath(os.path.join(base_path, rel_path))\n",
    "            page = img.get(\"page\", \"?\")\n",
    "\n",
    "            ctx.append(\n",
    "                f\"  Image {i}: {heading}\\n\"\n",
    "                f\"    • metadata_name: {meta_name}\\n\"\n",
    "                f\"    • page: {page}\\n\"\n",
    "                f\"    • file: {rel_path}\\n\"\n",
    "            )\n",
    "    else:\n",
    "        ctx.append(\"\\nNo images retrieved for this query.\\n\")\n",
    "\n",
    "    return \"\\n\".join(ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2858e9d",
   "metadata": {},
   "source": [
    "## 10. Build a focused LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3946d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_system_prompt(path) -> str:\n",
    "    \"\"\"Load system prompt text from a file.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    return \"\"\n",
    "\n",
    "def make_prompt(\n",
    "    query: str,\n",
    "    context: str,\n",
    "    plan: dict = None,\n",
    "    system_prompt_file: str = PATH_SYSTEM_PROMPT,\n",
    "    env=env,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a concise, instruction-style prompt for the LLM\n",
    "    Render operator assistant prompt using Jinja2 template and system prompt file.\n",
    "    dynamically setting min_images based on the retrieved plan.\"\"\"\n",
    "    \n",
    "    # Load system prompt text (extra safety / specialization rules)\n",
    "    system_prompt = load_system_prompt(system_prompt_file)\n",
    "\n",
    "    # Dynamically compute min_images based on retrieved context\n",
    "    #if plan and \"images\" in plan and len(plan[\"images\"]) > 0:\n",
    "    #    min_images = len(plan[\"images\"])\n",
    "    #else:\n",
    "    #    min_images = 1  # fallback to 1 as a baseline\n",
    "    min_images = getattr(cfg, \"min_images\", 2)\n",
    "\n",
    "    template = env.get_template(\"operator_prompt.j2\")\n",
    "    \n",
    "    return template.render(\n",
    "        query=query,\n",
    "        context=context,\n",
    "        min_images=min_images,\n",
    "        system_prompt=system_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ee470c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan retrieved:\n",
      "{\n",
      "  \"query\": \"I see in the panel in SIF402 a message: 'Please refill red pellets', what should I do?\",\n",
      "  \"steps\": [\n",
      "    \"Pellets\",\n",
      "    \"\\u201cPellet presence\\\": there are pellets present.\",\n",
      "    \"\\u201cMin level of pellet\\\": no pellets present.\"\n",
      "  ],\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"path\": \"artifacts/images/SIF02_p038_img0069.png\",\n",
      "      \"page\": 37,\n",
      "      \"bbox\": [\n",
      "        122.1500015259,\n",
      "        434.8299865723,\n",
      "        473.6099853516,\n",
      "        662.6799926758\n",
      "      ],\n",
      "      \"heading_path\": \"pellets required for production.\",\n",
      "      \"image_metadata_name\": \"SIF02_pellets required for production._image_1\",\n",
      "      \"source_pdf\": \"data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"artifacts/images/SIF02_p026_img0038.png\",\n",
      "      \"page\": 25,\n",
      "      \"bbox\": [\n",
      "        132.6000061035,\n",
      "        587.7600097656,\n",
      "        507.240020752,\n",
      "        630.3599853516\n",
      "      ],\n",
      "      \"heading_path\": \"1.8 Planned stops\",\n",
      "      \"image_metadata_name\": \"SIF02_1.8 Planned stops_image_2\",\n",
      "      \"source_pdf\": \"data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"artifacts/images/SIF02_p032_img0049.png\",\n",
      "      \"page\": 31,\n",
      "      \"bbox\": [\n",
      "        86.5500030518,\n",
      "        333.1399536133,\n",
      "        509.549987793,\n",
      "        588.3199462891\n",
      "      ],\n",
      "      \"heading_path\": \"Checking the availability of raw materials\",\n",
      "      \"image_metadata_name\": \"SIF02_Checking the availability of raw materials_image_1\",\n",
      "      \"source_pdf\": \"data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"artifacts/images/SIF02_p037_img0063.png\",\n",
      "      \"page\": 36,\n",
      "      \"bbox\": [\n",
      "        98.8249969482,\n",
      "        485.3199462891,\n",
      "        496.325012207,\n",
      "        567.0699462891\n",
      "      ],\n",
      "      \"heading_path\": \"because there are not enough pellets of the colour indicated. To continue, fill the corresponding hopper with pellets. To finish, press the RESET button on the main screen of the HMI panel.\",\n",
      "      \"image_metadata_name\": \"SIF02_because there are not enough pellets of the colour indicated. To continue, fill the corresponding hopper with pellets. To finish, press the RESET button on the main screen of the HMI panel._image_1\",\n",
      "      \"source_pdf\": \"data/pdfs/ManualOp-Modo Manual SIF400_merged_SIF402.pdf\"\n",
      "    }\n",
      "  ],\n",
      "  \"context_used_charlen\": 190\n",
      "}\n",
      "\n",
      "Query: I see in the panel in SIF402 a message: 'Please refill red pellets', what should I do?\n",
      "\n",
      "Relevant textual context or steps found:\n",
      "\n",
      "  Step 1: Pellets\n",
      "  Step 2: “Pellet presence\": there are pellets present.\n",
      "  Step 3: “Min level of pellet\": no pellets present.\n",
      "\n",
      "Relevant images found in manuals:\n",
      "\n",
      "  Image 1: pellets required for production.\n",
      "    • metadata_name: SIF02_pellets required for production._image_1\n",
      "    • page: 37\n",
      "    • file: artifacts/images/SIF02_p038_img0069.png\n",
      "\n",
      "  Image 2: 1.8 Planned stops\n",
      "    • metadata_name: SIF02_1.8 Planned stops_image_2\n",
      "    • page: 25\n",
      "    • file: artifacts/images/SIF02_p026_img0038.png\n",
      "\n",
      "  Image 3: Checking the availability of raw materials\n",
      "    • metadata_name: SIF02_Checking the availability of raw materials_image_1\n",
      "    • page: 31\n",
      "    • file: artifacts/images/SIF02_p032_img0049.png\n",
      "\n",
      "  Image 4: because there are not enough pellets of the colour indicated. To continue, fill the corresponding hopper with pellets. To finish, press the RESET button on the main screen of the HMI panel.\n",
      "    • metadata_name: SIF02_because there are not enough pellets of the colour indicated. To continue, fill the corresponding hopper with pellets. To finish, press the RESET button on the main screen of the HMI panel._image_1\n",
      "    • page: 36\n",
      "    • file: artifacts/images/SIF02_p037_img0063.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#query = \"Emergency stop procedure for station X\"\n",
    "query =f\"I see in the panel in {MACHINE_NAME} a message: 'Please refill red pellets', what should I do?\"\n",
    "plan = retrieve_plan(query, cfg)\n",
    "\n",
    "print(\"Plan retrieved:\")\n",
    "print(json.dumps(plan, indent=2))\n",
    "\n",
    "# Build full multimodal context + prompt\n",
    "context = make_context_from_plan(plan, base_path=\".\")\n",
    "\n",
    "# Render the prompt dynamically\n",
    "prompt = make_prompt(query, context, plan=plan)\n",
    "\n",
    "print(f\"\\n{context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0be48",
   "metadata": {},
   "source": [
    "Main wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d11ee96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"query\": \"I see in the panel in SIF402 a message: 'Please refill red pellets', what should I do?\",\n",
      "  \"steps\": [\n",
      "    \"Step 1: Check the pellet hopper for the presence of red pellets. If the hopper is empty, you need to refill it. (see image SIF02_p037_img0063.png)\",\n",
      "    \"Step 2: Fill the hopper with the required red pellets to ensure production can continue. (see image SIF02_p038_img0069.png)\",\n",
      "    \"Step 3: After refilling, go to the main screen of the HMI panel and press the RESET button to clear the message and resume operation. (see image SIF02_p037_img0063.png)\"\n",
      "  ],\n",
      "  \"images_used\": [\n",
      "    \"artifacts/images/SIF02_p037_img0063.png\",\n",
      "    \"artifacts/images/SIF02_p038_img0069.png\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Send to LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # or gpt-4-turbo / gpt-4o depending on your plan\n",
    "    temperature=0.2,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "raw_answer = response.choices[0].message.content.strip()\n",
    "print(raw_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
