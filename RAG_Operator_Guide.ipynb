{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4f4152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RAG MVP for Operator Training (text + images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b9924",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1319fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, io\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "from IPython.display import display\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain.schema import Document\n",
    "\n",
    "\n",
    "import hashlib\n",
    "import json, re\n",
    "from math import inf\n",
    "from jinja2 import Environment, FileSystemLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a37a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally, uncomment and run these installs:\n",
    "# !pip install --upgrade pip\n",
    "# !pip install pymupdf pdfminer.six pillow numpy pandas tqdm faiss-cpu\n",
    "# !pip install open_clip_torch torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# Optional (GPU):\n",
    "# !pip install open_clip_torch torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# Optional re-ranking or OCR extras:\n",
    "# !pip install rapidfuzz opencv-python\n",
    "# If you plan to call OpenAI for step generation:\n",
    "# !pip install openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0538a",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4c025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ENV_PATH = Path(\".env\")\n",
    "\n",
    "\n",
    "def load_env_file(path: Path = ENV_PATH) -> dict:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Environment file '{path}' not found.\")\n",
    "    env_vars = {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        for raw_line in fh:\n",
    "            line = raw_line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if \"=\" not in line:\n",
    "                continue\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip().strip('\"').strip(\"'\")\n",
    "            env_vars[key] = value\n",
    "    os.environ.update(env_vars)\n",
    "    return env_vars\n",
    "\n",
    "\n",
    "_ENV_CACHE = load_env_file()\n",
    "\n",
    "\n",
    "def env_get(key: str, *, cast, required: bool = True, default=None):\n",
    "    raw = os.environ.get(key)\n",
    "    if raw is None or raw == \"\":\n",
    "        if required:\n",
    "            raise ValueError(f\"Missing required environment variable '{key}' in {ENV_PATH}\")\n",
    "        return default\n",
    "    if cast is str:\n",
    "        return raw\n",
    "    try:\n",
    "        return cast(raw)\n",
    "    except Exception as exc:\n",
    "        raise ValueError(f\"Invalid value for '{key}': {raw}\") from exc\n",
    "\n",
    "\n",
    "def env_get_list(key: str, *, separator: str = \",\", required: bool = True):\n",
    "    raw = env_get(key, cast=str, required=required)\n",
    "    if raw is None:\n",
    "        return []\n",
    "    entries = [part.strip() for part in raw.split(separator)]\n",
    "    values = [part for part in entries if part]\n",
    "    if required and not values:\n",
    "        raise ValueError(f\"Environment variable '{key}' must contain at least one entry.\")\n",
    "    return values\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    pdf_dir: str = env_get(\"PDF_DIR\", cast=str)\n",
    "    out_dir: str = env_get(\"OUT_DIR\", cast=str)\n",
    "    images_dir_name: str = env_get(\"IMAGES_DIR_NAME\", cast=str)\n",
    "    catalog_json: str = env_get(\"CATALOG_JSON\", cast=str)\n",
    "\n",
    "    # OpenCLIP model settings\n",
    "    openclip_model: str = env_get(\"OPENCLIP_MODEL\", cast=str)\n",
    "    openclip_pretrained: str = env_get(\"OPENCLIP_PRETRAINED\", cast=str)\n",
    "    device: str = env_get(\"DEVICE\", cast=str)\n",
    "\n",
    "    # Indexing / retrieval\n",
    "    dims: int = env_get(\"DIMS\", cast=int)\n",
    "    top_k: int = env_get(\"TOP_K\", cast=int)\n",
    "    image_boost: float = env_get(\"IMAGE_BOOST\", cast=float)\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Prepare paths\n",
    "Path(cfg.pdf_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "(Path(cfg.out_dir) / cfg.images_dir_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f848a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 machine names. Primary: SIF400\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Settings ---\n",
    "MACHINE_NAMES = env_get_list(\"MACHINE_NAMES\")\n",
    "DEFAULT_MACHINE_NAME = env_get(\"DEFAULT_MACHINE_NAME\", cast=str, required=False)\n",
    "if DEFAULT_MACHINE_NAME:\n",
    "    ordered_names = [DEFAULT_MACHINE_NAME] + [name for name in MACHINE_NAMES if name != DEFAULT_MACHINE_NAME]\n",
    "    MACHINE_NAMES = ordered_names\n",
    "    MACHINE_NAME = DEFAULT_MACHINE_NAME\n",
    "else:\n",
    "    MACHINE_NAME = MACHINE_NAMES[0]\n",
    "\n",
    "OPENAI_API_KEY = env_get(\"OPENAI_API_KEY\", cast=str)\n",
    "PATH_SYSTEM_PROMPT = env_get(\"PATH_SYSTEM_PROMPT\", cast=str)\n",
    "TEMPLATES_DIR = env_get(\"TEMPLATES_DIR\", cast=str)\n",
    "env = Environment(loader=FileSystemLoader(TEMPLATES_DIR))\n",
    "\n",
    "LLM_MODEL = env_get(\"LLM_MODEL\", cast=str)\n",
    "LLM_TEMPERATURE = env_get(\"LLM_TEMPERATURE\", cast=float)\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = env_get(\"HF_HOME\", cast=str)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = env_get(\"TRANSFORMERS_CACHE\", cast=str)\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = env_get(\"HUGGINGFACE_HUB_CACHE\", cast=str)\n",
    "\n",
    "print(f\"Loaded {len(MACHINE_NAMES)} machine names. Primary: {MACHINE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc71b8",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78aedaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "LOGS_DIR = Path(\"logs\")\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_DEBUG_LOG_PATH = None\n",
    "\n",
    "def _get_debug_log_path() -> Path:\n",
    "    global _DEBUG_LOG_PATH\n",
    "    if _DEBUG_LOG_PATH is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        _DEBUG_LOG_PATH = LOGS_DIR / f\"debug_log_{timestamp}.jsonl\"\n",
    "    return _DEBUG_LOG_PATH\n",
    "\n",
    "def write_log_entry(section: str, function: str, payload):\n",
    "    record = {\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"section\": section,\n",
    "        \"function\": function,\n",
    "        \"data\": payload,\n",
    "    }\n",
    "    with open(_get_debug_log_path(), \"a\", encoding=\"utf-8\") as log_file:\n",
    "        json.dump(record, log_file)\n",
    "        log_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def norm_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def safe_fname(s: str) -> str:\n",
    "    s = norm_space(s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", s)\n",
    "    return s[:200]\n",
    "\n",
    "def heading_score(fontsize: float) -> float:\n",
    "    # Heuristic: larger font size -> higher heading score\n",
    "    return fontsize\n",
    "\n",
    "def build_heading_hierarchy(previous_headings: List[Dict[str, Any]], current: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    # Simplistic heuristic based on font sizes: if current fontsize is smaller, it could be a subheading\n",
    "    # Otherwise, it might reset the hierarchy\n",
    "    if not previous_headings:\n",
    "        return [current]\n",
    "    last = previous_headings[-1]\n",
    "    if current[\"fontsize\"] < last[\"fontsize\"]:\n",
    "        return previous_headings + [current]  # deeper level\n",
    "    else:\n",
    "        return [current]  # reset hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16f1ce",
   "metadata": {},
   "source": [
    "## 3. PDF Parsing (text blocks, font sizes, images, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc64449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_filter(bbox, page_height, ignore_top_pct=0.05, ignore_bottom_pct=0.05, min_size=40, image=None):\n",
    "    \"\"\"\n",
    "    Filters out unwanted regions/images:\n",
    "      - skip images too close to top/bottom (logos, footers)\n",
    "      - skip small icons/logos (width or height < min_size)\n",
    "      - skip completely black images\n",
    "    Returns True if the image should be kept.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    width, height = x1 - x0, y1 - y0\n",
    "\n",
    "    # 1️ Skip top/bottom zones\n",
    "    if y1 < ignore_top_pct * page_height or y0 > (1 - ignore_bottom_pct) * page_height:\n",
    "        return False\n",
    "\n",
    "    # 2️ Skip small icons\n",
    "    if width < min_size or height < min_size:\n",
    "        return False\n",
    "\n",
    "    # 3️ Skip fully black images\n",
    "    if image is not None:\n",
    "        arr = np.array(image.convert(\"L\"))  # grayscale\n",
    "        if np.mean(arr) < 5:  # near-black threshold\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751ead3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def extract_pdf(pdf_path: str, out_dir: str, images_dir_name: str,\n",
    "                MACHINE_NAME: str,\n",
    "                ignore_top_pct=0.05, ignore_bottom_pct=0.05, min_size=40) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extracts text blocks and images from a PDF.\n",
    "    Includes MACHINE_NAME in all image metadata and filenames.\n",
    "    Filters images by region_filter() to skip header/footer/small/black ones.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    items = []\n",
    "    img_counter_global = 0\n",
    "\n",
    "    for page_index in range(len(doc)):\n",
    "        page = doc[page_index]\n",
    "        page_height = page.rect.height\n",
    "        page_dict = page.get_text(\"dict\")  # contains blocks, lines, spans with fonts/sizes\n",
    "        blocks = page_dict.get(\"blocks\", [])\n",
    "\n",
    "        # Collect text spans with font sizes and bbox\n",
    "        text_items = []\n",
    "        for b in blocks:\n",
    "            if b[\"type\"] == 0:  # text block\n",
    "                bbox = b[\"bbox\"]\n",
    "                lines = b.get(\"lines\", [])\n",
    "                full_text = \"\"\n",
    "                max_font = 0.0\n",
    "                for ln in lines:\n",
    "                    for sp in ln.get(\"spans\", []):\n",
    "                        full_text += sp[\"text\"]\n",
    "                        if sp.get(\"size\", 0) > max_font:\n",
    "                            max_font = sp[\"size\"]\n",
    "                text_items.append({\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": norm_space(full_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")),\n",
    "                    \"fontsize\": max_font,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"page\": page_index\n",
    "                })\n",
    "\n",
    "            elif b[\"type\"] == 1:  # image block\n",
    "                bbox = b[\"bbox\"]\n",
    "                # extract image by id from page.get_images\n",
    "                # Multiple images may exist; we will grab the one whose bbox matches approximately\n",
    "                # Instead, use doc.extract_image for images listing; safer: use get_pixmap on bbox.\n",
    "                pix = page.get_pixmap(clip=fitz.Rect(*bbox), dpi=150)\n",
    "                img_bytes = pix.tobytes(\"png\")\n",
    "                img = Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "                #apply region filter:\n",
    "                if not region_filter(bbox, page_height,\n",
    "                                     ignore_top_pct=ignore_top_pct,\n",
    "                                     ignore_bottom_pct=ignore_bottom_pct,\n",
    "                                     min_size=min_size,\n",
    "                                     image=img):\n",
    "                    continue  # skip unwanted images\n",
    "\n",
    "                # Save image with machine name prefix\n",
    "                img_counter_global += 1\n",
    "                img_name = f\"{MACHINE_NAME}_p{page_index+1:03d}_img{img_counter_global:04d}.png\"\n",
    "                img_path = os.path.join(out_dir, images_dir_name, img_name)\n",
    "                img.save(img_path)\n",
    "\n",
    "                text_items.append({\n",
    "                    \"type\": \"image\",\n",
    "                    \"image_path\": img_path,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"page\": page_index,\n",
    "                })\n",
    "\n",
    "        # Sort items by vertical position (y0)\n",
    "        text_items.sort(key=lambda x: x[\"bbox\"][1] if \"bbox\" in x else 0.0)\n",
    "\n",
    "        # Build heading contexts and attach nearest preceding headings to images\n",
    "        heading_stack: List[Dict[str, Any]] = []\n",
    "        image_counters_by_heading: Dict[str, int] = {}\n",
    "\n",
    "        for it in text_items:\n",
    "            if it[\"type\"] == \"text\":\n",
    "                # Heuristic: treat as heading if font size is among the largest on page or if it matches numbered pattern\n",
    "                is_numbered = bool(re.match(r\"^\\\\d+(\\\\.\\\\d+)*\\\\s+.+\", it[\"text\"]))\n",
    "                if is_numbered or it[\"fontsize\"] >= (max([t[\"fontsize\"] for t in text_items if t[\"type\"]==\"text\"]+[0])*0.9):\n",
    "                    heading_stack = build_heading_hierarchy(heading_stack, it)\n",
    "                    #Keep only meaningful text\n",
    "                    heading_stack[-1][\"text\"] = it[\"text\"]\n",
    "\n",
    "            elif it[\"type\"] == \"image\":\n",
    "                # Build heading chain text\n",
    "                if heading_stack:\n",
    "                    chain_texts = [h[\"text\"] for h in heading_stack if h.get(\"text\")]\n",
    "                    chain_compact = \" > \".join(chain_texts)\n",
    "                    top_heading = heading_stack[-1][\"text\"]\n",
    "                else:\n",
    "                    chain_texts = []\n",
    "                    chain_compact = \"\"\n",
    "                    top_heading = \"Unlabeled\"\n",
    "\n",
    "                # Create metadata key for counting images under the top heading\n",
    "                top_key = safe_fname(top_heading) if top_heading else \"Unlabeled\"\n",
    "                image_counters_by_heading.setdefault(top_key, 0)\n",
    "                image_counters_by_heading[top_key] += 1\n",
    "                img_idx = image_counters_by_heading[top_key]\n",
    "\n",
    "                # Build user-style metadata name\n",
    "                # Prefer the deepest numbered heading if available\n",
    "                numbered = [t for t in chain_texts if re.match(r\"^\\\\d+(\\\\.\\\\d+)*\\\\s+.+\", t)]\n",
    "                final_heading = numbered[-1] if numbered else top_heading or \"Unlabeled\"\n",
    "\n",
    "                image_metadata_name = f\"{MACHINE_NAME}_{final_heading}_image_{img_idx}\"\n",
    "\n",
    "                items.append({\n",
    "                    \"modality\": \"image\",\n",
    "                    \"page\": it[\"page\"],\n",
    "                    \"bbox\": it[\"bbox\"],\n",
    "                    \"image_path\": it[\"image_path\"],\n",
    "                    \"heading_chain\": chain_texts,\n",
    "                    \"heading_path\": chain_compact,\n",
    "                    \"image_metadata_name\": image_metadata_name,\n",
    "                    \"source_pdf\": pdf_path,\n",
    "                    \"source_machine\": MACHINE_NAME\n",
    "                })\n",
    "\n",
    "        # Record text blocks for text retrieval\n",
    "        for t in text_items:\n",
    "            if t[\"type\"] == \"text\" and t.get(\"text\"):\n",
    "                items.append({\n",
    "                    \"modality\": \"text\",\n",
    "                    \"page\": t[\"page\"],\n",
    "                    \"bbox\": t[\"bbox\"],\n",
    "                    \"text\": t[\"text\"],\n",
    "                    \"fontsize\": t[\"fontsize\"],\n",
    "                    \"source_pdf\": pdf_path,\n",
    "                    \"source_machine\": MACHINE_NAME\n",
    "                })\n",
    "\n",
    "    doc.close()\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b5336",
   "metadata": {},
   "source": [
    "### 3.3 Extract text and images from pdfs to build the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd3adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(pdf_dir: str, out_dir: str, images_dir_name: str, catalog_json: str,\n",
    "                    MACHINE_NAMES: List[str], ignore_bottom_pct: float, ignore_top_pct: float):\n",
    "    machine_names = []\n",
    "    for name in MACHINE_NAMES:\n",
    "        cleaned = norm_space(name)\n",
    "        if cleaned:\n",
    "            machine_names.append(cleaned)\n",
    "    if not machine_names:\n",
    "        raise ValueError(\"MACHINE_NAMES must include at least one non-empty name.\")\n",
    "\n",
    "    all_items = []\n",
    "    items_per_machine: Dict[str, int] = {name: 0 for name in machine_names}\n",
    "    pdfs = sorted(Path(pdf_dir).glob(\"**/*.pdf\"))\n",
    "    unmatched_pdfs = []\n",
    "    progress = tqdm(pdfs, desc=\"Extracting PDFs\")\n",
    "\n",
    "    for pdf_path in progress:\n",
    "        pdf_path = Path(pdf_path)\n",
    "        stem_upper = pdf_path.stem.upper()\n",
    "        matched_name = None\n",
    "        for candidate in machine_names:\n",
    "            if stem_upper.startswith(candidate.upper()):\n",
    "                matched_name = candidate\n",
    "                break\n",
    "\n",
    "        if matched_name is None:\n",
    "            progress.set_description(\"Extracting PDFs\")\n",
    "            unmatched_pdfs.append(str(pdf_path))\n",
    "            continue\n",
    "\n",
    "        progress.set_description(f\"Extracting {matched_name}\")\n",
    "        items = extract_pdf(\n",
    "            str(pdf_path),\n",
    "            out_dir,\n",
    "            images_dir_name,\n",
    "            matched_name,\n",
    "            ignore_top_pct=ignore_top_pct,\n",
    "            ignore_bottom_pct=ignore_bottom_pct,\n",
    "        )\n",
    "        all_items.extend(items)\n",
    "        items_per_machine[matched_name] = items_per_machine.get(matched_name, 0) + len(items)\n",
    "\n",
    "    progress.set_description(\"Extracting PDFs\")\n",
    "\n",
    "    if unmatched_pdfs:\n",
    "        print(\"Skipped PDFs without matching machine name:\")\n",
    "        for skipped in unmatched_pdfs:\n",
    "            print(f\"  - {skipped}\")\n",
    "\n",
    "    out_path = os.path.join(out_dir, catalog_json)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for it in all_items:\n",
    "            line = json.dumps(it, ensure_ascii=False)\n",
    "            line = line.replace(\"\\n\", \"\\\\n\")  # escape accidental newlines inside JSON\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(all_items)} items to {out_path}\")\n",
    "    for name in machine_names:\n",
    "        count = items_per_machine.get(name, 0)\n",
    "        if count:\n",
    "            print(f\"  {name}: {count} items\")\n",
    "\n",
    "    return all_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b532c7",
   "metadata": {},
   "source": [
    "## 4. OpenCLIP: Model & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbabf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache directory: /tmp/hf_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Confirm effective cache directory\n",
    "print(\"Hugging Face cache directory:\", os.environ[\"HF_HOME\"])\n",
    "\n",
    "# Lazy import to avoid failures if packages aren't installed yet\n",
    "def load_openclip(model_name: str, pretrained: str, device: str=\"cpu\"):\n",
    "    import torch\n",
    "    import open_clip\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "                model_name, pretrained=pretrained, device=device,\n",
    "                cache_dir=os.environ[\"HF_HOME\"]   # ensure it uses the safe cache\n",
    "    )\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    return model, preprocess, tokenizer\n",
    "\n",
    "def embed_texts_openclip(texts: List[str], model, tokenizer, device=\"cpu\", batch_size=32) -> np.ndarray:\n",
    "    import torch\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            tok = tokenizer(batch)\n",
    "            tok = {k: v.to(device) for k,v in tok.items()} if isinstance(tok, dict) else tok.to(device)\n",
    "            feats = model.encode_text(tok)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            embs.append(feats.cpu().numpy())\n",
    "    return np.vstack(embs) if embs else np.zeros((0,))\n",
    "\n",
    "def embed_images_openclip(img_paths: List[str], model, preprocess, device=\"cpu\", batch_size=16) -> np.ndarray:\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(img_paths), batch_size):\n",
    "            batch = img_paths[i:i+batch_size]\n",
    "            imgs = []\n",
    "            for p in batch:\n",
    "                im = Image.open(p).convert(\"RGB\")\n",
    "                imgs.append(preprocess(im))\n",
    "            imgs = torch.stack(imgs).to(device)\n",
    "            feats = model.encode_image(imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            embs.append(feats.cpu().numpy())\n",
    "    return np.vstack(embs) if embs else np.zeros((0,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3e891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############Captions#############\n",
    "# Build a textual surrogate (caption) since images in pdfs don't come with captions\n",
    "import pandas as pd\n",
    "def build_caption_for_images(df_img: pd.DataFrame) -> List[str]:\n",
    "    # Build a textual surrogate (caption)\n",
    "    captions = []\n",
    "    for rec in df_img.to_dict(orient=\"records\"):\n",
    "        # Combine semantic hints into a short phrase\n",
    "        meta_parts = [\n",
    "            rec.get(\"image_metadata_name\") or \"\",\n",
    "            rec.get(\"heading_path\") or \"\",\n",
    "        ]\n",
    "        caption = \" | \".join(p for p in meta_parts if p)\n",
    "        captions.append(caption if caption.strip() else \"image from manual\")\n",
    "    return captions\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d40be",
   "metadata": {},
   "source": [
    "## 5. Build a Multimodal FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf1ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def json_parser(catalog_path: str) -> pd.DataFrame:\n",
    "    # Load catalog\n",
    "    rows = []\n",
    "    with open(catalog_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            #Try safe parsing — skip malformed lines\n",
    "            try:\n",
    "                # Some lines may accidentally contain multiple JSON objects -> split them\n",
    "                parts = re.findall(r'\\{.*?\\}(?=$|\\s*\\{)', line)\n",
    "                if len(parts) > 1:\n",
    "                    for p in parts:\n",
    "                        rows.append(json.loads(p))\n",
    "                else:\n",
    "                    rows.append(json.loads(line))\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skipping bad JSON line {i}: {str(e)[:80]}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Loaded {len(rows)} valid JSON objects from {catalog_path}\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(df.head(2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbad95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "def build_indices(catalog_path: str, cfg: Config):\n",
    "    # Load catalog    \n",
    "    df = json_parser(catalog_path)\n",
    "\n",
    "    # Separate modalities\n",
    "    df_text = df[df[\"modality\"]==\"text\"].copy()\n",
    "    df_img = df[df[\"modality\"]==\"image\"].copy()\n",
    "\n",
    "    # Load OpenCLIP\n",
    "    model, preprocess, tokenizer = load_openclip(cfg.openclip_model, cfg.openclip_pretrained, cfg.device)\n",
    "\n",
    "    # Prepare contents for embeddings\n",
    "    text_payloads = df_text[\"text\"].tolist()\n",
    "    img_paths = df_img[\"image_path\"].tolist()\n",
    "\n",
    "    print(f\"Embedding {len(text_payloads)} text blocks and {len(img_paths)} images...\")\n",
    "    text_embs = embed_texts_openclip(text_payloads, model, tokenizer, cfg.device)\n",
    "    img_embs = embed_images_openclip(img_paths, model, preprocess, cfg.device)\n",
    "\n",
    "\n",
    "###########New###########################\n",
    "    # Create text embeddings for captions using the same text encoder\n",
    "    captions = build_caption_for_images(df_img)\n",
    "    text_capt_embs = embed_texts_openclip(captions, model, tokenizer, cfg.device)\n",
    "\n",
    "    # Normalize both before fusing\n",
    "    img_embs = img_embs / np.linalg.norm(img_embs, axis=1, keepdims=True)\n",
    "    text_capt_embs = text_capt_embs / np.linalg.norm(text_capt_embs, axis=1, keepdims=True)\n",
    "\n",
    "    # Weighted fusion (adjust alpha to taste)\n",
    "    alpha = 0.5  # try 0.3–0.7 range\n",
    "    emb_fused = (1 - alpha) * img_embs + alpha * text_capt_embs\n",
    "    emb_fused = emb_fused / np.linalg.norm(emb_fused, axis=1, keepdims=True)\n",
    "\n",
    "    # 5. Use emb_fused for indexing\n",
    "    #index_image.add(emb_fused.astype(\"float32\"))\n",
    "\n",
    "    print(\"Image-only mean sim:\", np.mean(img_embs @ text_embs.T))\n",
    "    print(\"Fused mean sim:\", np.mean(emb_fused @ text_embs.T))\n",
    "\n",
    "    # Replace img_embs with fused embeddings\n",
    "    img_embs = emb_fused\n",
    "    ########################################\n",
    "\n",
    "\n",
    "\n",
    "    # Build FAISS indices (cosine similarity via inner product on normalized vectors)\n",
    "    dim_text = text_embs.shape[1] if len(text_embs.shape)==2 else cfg.dims\n",
    "    dim_img  = img_embs.shape[1] if len(img_embs.shape)==2 else cfg.dims\n",
    "\n",
    "    index_text = faiss.IndexFlatIP(dim_text)\n",
    "    index_img  = faiss.IndexFlatIP(dim_img)\n",
    "\n",
    "    if len(text_embs):\n",
    "        index_text.add(text_embs.astype(\"float32\"))\n",
    "    if len(img_embs):\n",
    "        index_img.add(img_embs.astype(\"float32\"))\n",
    "\n",
    "    # Persist\n",
    "    faiss.write_index(index_text, os.path.join(cfg.out_dir, \"faiss_text.index\"))\n",
    "    faiss.write_index(index_img,  os.path.join(cfg.out_dir, \"faiss_image.index\"))\n",
    "    df_text.to_json(os.path.join(cfg.out_dir, \"df_text.json\"), orient=\"records\", lines=True)\n",
    "    df_img.to_json(os.path.join(cfg.out_dir, \"df_img.json\"), orient=\"records\", lines=True)\n",
    "\n",
    "    print(\"Indices built and saved.\")\n",
    "    return {\n",
    "        \"index_text_path\": os.path.join(cfg.out_dir, \"faiss_text.index\"),\n",
    "        \"index_image_path\": os.path.join(cfg.out_dir, \"faiss_image.index\"),\n",
    "        \"df_text_path\": os.path.join(cfg.out_dir, \"df_text.json\"),\n",
    "        \"df_image_path\": os.path.join(cfg.out_dir, \"df_img.json\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b266d",
   "metadata": {},
   "source": [
    "## 6. Retrieval & Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6061c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "################New\n",
    "# --- Normalize scores per modality\n",
    "def normalize_scores(recs):\n",
    "    if not recs: return recs\n",
    "    vals = np.array([r[\"score\"] for r in recs], dtype=float)\n",
    "    mu, sigma = vals.mean(), vals.std() + 1e-6\n",
    "    for r in recs:\n",
    "        r[\"score_norm\"] = (r[\"score\"] - mu) / sigma\n",
    "    return recs\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "220f3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indices(cfg: Config):\n",
    "    import pandas as pd\n",
    "    index_text = faiss.read_index(os.path.join(cfg.out_dir, \"faiss_text.index\"))\n",
    "    index_image = faiss.read_index(os.path.join(cfg.out_dir, \"faiss_image.index\"))\n",
    "    df_text = pd.read_json(os.path.join(cfg.out_dir, \"df_text.json\"), orient=\"records\", lines=True)\n",
    "    df_img  = pd.read_json(os.path.join(cfg.out_dir, \"df_img.json\"),  orient=\"records\", lines=True)\n",
    "    model, preprocess, tokenizer = load_openclip(cfg.openclip_model, cfg.openclip_pretrained, cfg.device)\n",
    "    return index_text, index_image, df_text, df_img, model, preprocess, tokenizer\n",
    "\n",
    "def search(query: str, cfg: Config, top_k: int=None) -> Dict[str, Any]:\n",
    "    import torch\n",
    "    top_k = top_k or cfg.top_k\n",
    "    index_text, index_image, df_text, df_img, model, preprocess, tokenizer = load_indices(cfg)\n",
    "\n",
    "    # Embed query\n",
    "    q_emb = embed_texts_openclip([query], model, tokenizer, cfg.device)[0].astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "    # Search text\n",
    "    D_t, I_t = index_text.search(q_emb, top_k)\n",
    "    # Search images\n",
    "    D_i, I_i = index_image.search(q_emb, top_k)\n",
    "\n",
    "    # Combine with simple late fusion (scores already cosine similarities)\n",
    "    #results = []\n",
    "    text_hits = []\n",
    "    img_hits = []\n",
    "    \n",
    "    for score, idx in zip(D_t[0].tolist(), I_t[0].tolist()):\n",
    "        if idx == -1: continue\n",
    "        rec = df_text.iloc[idx].to_dict()\n",
    "        rec.update({\"score\": float(score), \"modality\": \"text\"})\n",
    "        ##results.append(rec)\n",
    "        text_hits.append(rec)\n",
    "\n",
    "    for score, idx in zip(D_i[0].tolist(), I_i[0].tolist()):\n",
    "        if idx == -1: continue\n",
    "        rec = df_img.iloc[idx].to_dict()\n",
    "        rec.update({\"score\": float(score * cfg.image_boost), \"modality\": \"image\"})\n",
    "        #results.append(rec)\n",
    "        img_hits.append(rec)\n",
    "\n",
    "    # Sort by score desc and take top_k overall\n",
    "    #results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    #results = results[:top_k]\n",
    "    \n",
    "    # --- Normalize scores per modality\n",
    "    text_hits = normalize_scores(text_hits)\n",
    "    img_hits = normalize_scores(img_hits)\n",
    "\n",
    "    # --- Merge and sort by normalized score ---\n",
    "    merged = text_hits + img_hits\n",
    "    merged.sort(key=lambda x: x[\"score_norm\"], reverse=True)\n",
    "    results = merged[:top_k]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d20a2",
   "metadata": {},
   "source": [
    "## 7. JSON Output Composer (Steps + Relevant Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15daec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_steps_from_text(text: str) -> List[str]:\n",
    "    # Simple heuristic: split on numbered or bulleted lines; clean\n",
    "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "    steps = []\n",
    "    for ln in lines:\n",
    "        if re.match(r\"^(?:\\d+\\.|[-*•])\\s+\", ln):\n",
    "            steps.append(re.sub(r\"^(?:\\d+\\.|[-*•])\\s+\", \"\", ln).strip())\n",
    "    # Fallback: if no explicit bullets, chunk sentences\n",
    "    if not steps:\n",
    "        sents = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        steps = [s.strip() for s in sents if len(s.strip()) > 0][:6]\n",
    "    return steps[:8]\n",
    "\n",
    "def compose_json_plan(query: str, search_out: Dict[str, Any], max_images: int = 4) -> Dict[str, Any]:\n",
    "    # Gather top text to form step suggestions, and top images to include\n",
    "    texts = [r[\"text\"] for r in search_out[\"results\"] if r[\"modality\"]==\"text\" and isinstance(r.get(\"text\"), str)]\n",
    "    big_context = \"\\n\".join(texts[:5]) if texts else \"\"\n",
    "    steps = extract_steps_from_text(big_context) if big_context else []\n",
    "\n",
    "    img_hits = [r for r in search_out[\"results\"] if r[\"modality\"]==\"image\"]\n",
    "    img_hits = img_hits[:max_images]\n",
    "\n",
    "    images_payload = []\n",
    "    for r in img_hits:\n",
    "        images_payload.append({\n",
    "            \"path\": r.get(\"image_path\"),\n",
    "            \"page\": int(r.get(\"page\", -1)),\n",
    "            \"bbox\": r.get(\"bbox\"),\n",
    "            \"heading_path\": r.get(\"heading_path\"),\n",
    "            \"image_metadata_name\": r.get(\"image_metadata_name\"),\n",
    "            \"source_pdf\": r.get(\"source_pdf\")\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"steps\": steps,\n",
    "        \"images\": images_payload,\n",
    "        \"context_used_charlen\": len(big_context)\n",
    "    }\n",
    "\n",
    "# Example usage (after building the index):\n",
    "# out = search(\"Emergency stop procedure\", cfg, top_k=10)\n",
    "# plan = compose_json_plan(\"Emergency stop procedure\", out, max_images=4)\n",
    "# plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9d418",
   "metadata": {},
   "source": [
    "## 8. Builds the PDF Extraction, parsing and Embeddings Index Creation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9586eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SIF413: 100% 14/14 [00:39<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2208 items to artifacts/catalog.jsonl\n",
      "  SIF400: 404 items\n",
      "  SIF401: 131 items\n",
      "  SIF402: 187 items\n",
      "  SIF403: 167 items\n",
      "  SIF404: 125 items\n",
      "  SIF405: 144 items\n",
      "  SIF406: 138 items\n",
      "  SIF407: 126 items\n",
      "  SIF408: 218 items\n",
      "  SIF409: 108 items\n",
      "  SIF410: 173 items\n",
      "  SIF411: 110 items\n",
      "  SIF412: 119 items\n",
      "  SIF413: 58 items\n",
      "Loaded 2208 valid JSON objects from artifacts/catalog.jsonl\n",
      "  modality  page                                               bbox  \\\n",
      "0    image     0  [38.375003814697266, 164.77999877929688, 80.85...   \n",
      "1     text     0  [352.1499938964844, 210.88999938964844, 466.23...   \n",
      "\n",
      "                                 image_path heading_chain heading_path  \\\n",
      "0  artifacts/images/SIF400_p001_img0001.png            []                \n",
      "1                                       NaN           NaN          NaN   \n",
      "\n",
      "        image_metadata_name                                      source_pdf  \\\n",
      "0  SIF400_Unlabeled_image_1  data/pdfs/SIF400_ManualOp_CommonProcedures.pdf   \n",
      "1                       NaN  data/pdfs/SIF400_ManualOp_CommonProcedures.pdf   \n",
      "\n",
      "  source_machine         text  fontsize  \n",
      "0         SIF400          NaN       NaN  \n",
      "1         SIF400  Manual mode      18.0  \n",
      "Embedding 1824 text blocks and 384 images...\n",
      "Image-only mean sim: 0.19196495\n",
      "Fused mean sim: 0.46969926\n",
      "Indices built and saved.\n",
      "2) {'index_text_path': 'artifacts/faiss_text.index', 'index_image_path': 'artifacts/faiss_image.index', 'df_text_path': 'artifacts/df_text.json', 'df_image_path': 'artifacts/df_img.json'} # may take time\n"
     ]
    }
   ],
   "source": [
    "def build_all(cfg: Config, machines_list: List):\n",
    "    catalog_path = os.path.join(cfg.out_dir, cfg.catalog_json)\n",
    "    _ = run_extraction(\n",
    "        cfg.pdf_dir,\n",
    "        cfg.out_dir,\n",
    "        cfg.images_dir_name,\n",
    "        cfg.catalog_json,\n",
    "        MACHINE_NAMES=machines_list,\n",
    "        ignore_bottom_pct=0.1,\n",
    "        ignore_top_pct=0.1,\n",
    "    )\n",
    "    idx_paths = build_indices(catalog_path, cfg)\n",
    "    return idx_paths\n",
    "\n",
    "print(f\"2) {build_all(cfg, MACHINE_NAMES)} # may take time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba22c78",
   "metadata": {},
   "source": [
    "### 8.1 Retrieves the execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766488d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_plan(query: str, cfg: Config, top_k: int=10, max_images: int=4):\n",
    "    search_out = search(query, cfg, top_k=top_k)\n",
    "    plan = compose_json_plan(query, search_out, max_images=max_images)\n",
    "\n",
    "\n",
    "    write_log_entry(\n",
    "        \"8. End-to-End Runner\",\n",
    "        \"retrieve_plan\",\n",
    "        {\n",
    "            \"instructions\": [\n",
    "                f\"Put PDFs in: {cfg.pdf_dir}\",\n",
    "                \"Run build_all(cfg) # may take time\",\n",
    "                f\"retrieve_plan(query='{query}', cfg)\",\n",
    "            ],\n",
    "            \"query\": query,\n",
    "            \"plan\": plan,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ec586",
   "metadata": {},
   "source": [
    "### 8.2 End-to-End Runner RUN THIS SECTION ONLY FOR TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For DEBUGGING:\n",
    "#print(\"To run:\")\n",
    "#print(\"1) Put PDFs in:\", cfg.pdf_dir)\n",
    "#print(f\"2) {build_all(cfg)} # may take time\")\n",
    "#print(f\"3) {retrieve_plan('Emergency stop procedure for station X', cfg)}\")\n",
    "#query =f\"I see in the panel in {MACHINE_NAME} a message: 'Please refill red pellets', what should I do?\"\n",
    "#query =f\"What can you tell me about machine SIF401?\"\n",
    "#plan = retrieve_plan(query, cfg)\n",
    "#print(f\"3) {plan}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ee6ca",
   "metadata": {},
   "source": [
    "## 9. Build a context that merges text + image info\n",
    "\n",
    "It creates a single human-readable context string summarizing everything retrieved for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "400932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_context_from_plan(plan: dict, base_path: str = \".\") -> str:\n",
    "    \"\"\"\n",
    "    Build a readable text context for the LLM that includes:\n",
    "    - Textual context / steps retrieved\n",
    "    - References to relevant images with true metadata names and relative paths\n",
    "    \n",
    "    Args:\n",
    "        plan: dict returned by retrieve_plan()\n",
    "        base_path: root folder (e.g. project Basepath), used to resolve image paths\n",
    "    \n",
    "    Returns:\n",
    "        str: context string for LLM prompt\n",
    "    \"\"\"\n",
    "    ctx = []\n",
    "    ctx.append(f\"Query: {plan.get('query', '')}\\n\")\n",
    "\n",
    "    # ---- TEXT CONTEXT ----\n",
    "    if plan.get(\"steps\"):\n",
    "        ctx.append(\"Relevant textual context or steps found:\\n\")\n",
    "        for i, step in enumerate(plan[\"steps\"], start=1):\n",
    "            ctx.append(f\"  Step {i}: {step}\")\n",
    "    else:\n",
    "        ctx.append(\"No textual steps found in retrieval.\\n\")\n",
    "\n",
    "    # ---- IMAGE CONTEXT ----\n",
    "    images = plan.get(\"images\", [])\n",
    "    if images:\n",
    "        ctx.append(\"\\nRelevant images found in manuals:\\n\")\n",
    "        for i, img in enumerate(images, start=1):\n",
    "            heading = img.get(\"heading_path\", \"Unlabeled section\")\n",
    "            meta_name = img.get(\"image_metadata_name\", \"Unknown_image\")\n",
    "            rel_path = img.get(\"path\", \"\")\n",
    "            abs_path = os.path.abspath(os.path.join(base_path, rel_path))\n",
    "            page = img.get(\"page\", \"?\")\n",
    "\n",
    "            ctx.append(\n",
    "                f\"  Image {i}: {heading}\\n\"\n",
    "                f\"    • metadata_name: {meta_name}\\n\"\n",
    "                f\"    • page: {page}\\n\"\n",
    "                f\"    • file: {rel_path}\\n\"\n",
    "            )\n",
    "    else:\n",
    "        ctx.append(\"\\nNo images retrieved for this query.\\n\")\n",
    "\n",
    "    return \"\\n\".join(ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2858e9d",
   "metadata": {},
   "source": [
    "## 10. Build a focused LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3946d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_system_prompt(path) -> str:\n",
    "    \"\"\"Load system prompt text from a file.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    return \"\"\n",
    "\n",
    "def make_prompt(\n",
    "    query: str,\n",
    "    context: str,\n",
    "    system_prompt_file: str = PATH_SYSTEM_PROMPT,\n",
    "    env=env,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a concise, instruction-style prompt for the LLM.\n",
    "    Renders the operator assistant prompt using Jinja2 template and system prompt file,\n",
    "    dynamically injecting context-specific focus rules when appropriate.\"\"\"\n",
    "    \n",
    "    # Load system prompt text (extra safety / specialization rules)\n",
    "    system_prompt = load_system_prompt(system_prompt_file)\n",
    "\n",
    "    # Dynamically compute min_images based on retrieved context\n",
    "    #if plan and \"images\" in plan and len(plan[\"images\"]) > 0:\n",
    "    #    min_images = len(plan[\"images\"])\n",
    "    #else:\n",
    "    #    min_images = 1  # fallback to 1 as a baseline\n",
    "    min_images = getattr(cfg, \"min_images\", 3)\n",
    "    min_steps = getattr(cfg, \"min_steps\", 3)\n",
    "\n",
    "    # --- NEW LOGIC: Inject dynamic focus reminder based on query ---\n",
    "    # Detect whether this query is about a specific machine, station, or operational behavior\n",
    "    machine_keywords = [\n",
    "        \"station\", \"machine\", \"module\", \"unit\", \"cell\",\n",
    "        \"manual mode\", \"operation\", \"configure\", \"function\", \"procedure\"\n",
    "    ]\n",
    "\n",
    "    # Lowercase match to reduce brittleness\n",
    "    if any(k in query.lower() for k in machine_keywords):\n",
    "        context_focus_reminder = \"\"\"\n",
    "        Before answering, ensure you analyze and understand the following sections\n",
    "        for the relevant machine or station (if present in the context):\n",
    "\n",
    "        1. \"Station Function\" — what the station does and its role in the process.\n",
    "        2. \"Station Operation\" — how the operator interacts with it.\n",
    "        3. \"Configuring the Station in Manual Mode\" — how to safely prepare or test it.\n",
    "\n",
    "        Base your reasoning primarily on these sections before considering any\n",
    "        other parts of the manual. Only then integrate additional or more specific\n",
    "        steps related to the user’s query.\n",
    "        \"\"\"\n",
    "        system_prompt += \"\\n\\n\" + context_focus_reminder\n",
    "    \n",
    "    \n",
    "    template = env.get_template(\"operator_prompt.j2\")\n",
    "    \n",
    "    prompt = template.render(\n",
    "        query=query,\n",
    "        context=context,\n",
    "        min_images=min_images,\n",
    "        min_steps=min_steps,\n",
    "        system_prompt=system_prompt,\n",
    "    )\n",
    "\n",
    "    # Add a final reminder to prevent extra formatting\n",
    "    prompt += \"\\n\\nIMPORTANT: Respond ONLY with a valid JSON object. Do not use Markdown code fences or explanations.\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ee470c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#query = \"Emergency stop procedure for station X\"\n",
    "#query =f\"I see in the panel in {MACHINE_NAME} a message: 'Please refill red pellets', what should I do?\"\n",
    "#query =f\"I see a round canister in front of the {MACHINE_NAME} and I want to fill it with yello pellets, what should I do next?\"\n",
    "\n",
    "def answer_query(query: str)-> str:\n",
    "    plan = retrieve_plan(query, cfg)\n",
    "    \n",
    "    # Build full multimodal context + prompt\n",
    "    context = make_context_from_plan(plan, base_path=\".\")\n",
    "\n",
    "    #For Debug:\n",
    "    #print(\"Plan retrieved:\")\n",
    "    #print(json.dumps(plan, indent=2))\n",
    "    #print(\"Context:\")\n",
    "    #print(f\"{context}\")\n",
    "    ####\n",
    "\n",
    "    # Render the prompt dynamically\n",
    "    prompt = make_prompt(query, context)\n",
    "\n",
    "    write_log_entry(\n",
    "        \"10. Build a focused LLM prompt\",\n",
    "        \"make_prompt\",\n",
    "        {\n",
    "            \"query\": query,\n",
    "            \"plan\": plan,\n",
    "            \"context\": context,\n",
    "            \"prompt\": prompt,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "\n",
    "    # print(\"=== Rendered Prompt ===\")\n",
    "    # print(prompt)\n",
    "    # print(\"=======================\")\n",
    "\n",
    "\n",
    "\n",
    "    # Send to LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or gpt-4-turbo / gpt-4o depending on your plan\n",
    "        temperature=0.2,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    raw_answer = response.choices[0].message.content.strip()\n",
    "    write_log_entry(\n",
    "        \"Main wrapper\",\n",
    "        \"client.chat.completions.create\",\n",
    "        {\"response\": raw_answer},\n",
    "    )\n",
    "    \n",
    "    # Try to extract JSON between braces if the model added text\n",
    "    if \"{\" in raw_answer and \"}\" in raw_answer:\n",
    "        json_part = raw_answer[raw_answer.find(\"{\") : raw_answer.rfind(\"}\") + 1]\n",
    "        try:\n",
    "            json.loads(json_part)\n",
    "            raw_answer = json_part\n",
    "        except Exception:\n",
    "            pass  # leave as-is if still invalid\n",
    "\n",
    "    \n",
    "    return raw_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0be48",
   "metadata": {},
   "source": [
    "Main wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9378c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"Can you describe to operator what is the function of machine SIF401?\",\n",
      "  \"steps\": [\n",
      "    \"Step 1: Understand that the SIF-400 system has five stations that assess production quality. (see image artifacts/images/SIF410_p002_img0002.png)\",\n",
      "    \"Step 2: Note that the SIF-410 station operates in two modes: manual mode and integrated mode. (see image artifacts/images/SIF405_p002_img0002.png)\",\n",
      "    \"Step 3: Recognize that to operate in integrated mode, the SIFMES-400 operation software must be run. (see image artifacts/images/SIF406_p002_img0002.png)\"\n",
      "  ],\n",
      "  \"images_used\": [\n",
      "    \"artifacts/images/SIF410_p002_img0002.png\",\n",
      "    \"artifacts/images/SIF405_p002_img0002.png\",\n",
      "    \"artifacts/images/SIF406_p002_img0002.png\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#query =f\"I see a round canister in front of the {MACHINE_NAME} and I want to fill it with yello pellets, what should I do next?\"\n",
    "#query =f\"I want to use the machine SIF405 to automatically pu a lid to a round container, what should I do?\"\n",
    "#query =f\"How do I start up machine SIF 405?\"\n",
    "query =f\"Can you describe to operator what is the funtion of machine SIF401?\"\n",
    "print (answer_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d11ee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Send to LLM\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-4o-mini\",  # or gpt-4-turbo / gpt-4o depending on your plan\n",
    "#     temperature=0.2,\n",
    "#     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "# )\n",
    "\n",
    "# raw_answer = response.choices[0].message.content.strip()\n",
    "# write_log_entry(\n",
    "#     \"Main wrapper\",\n",
    "#     \"client.chat.completions.create\",\n",
    "#     {\"response\": raw_answer},\n",
    "# )\n",
    "# print(raw_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
